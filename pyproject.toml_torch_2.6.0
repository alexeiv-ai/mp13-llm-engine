[tool.poetry] 
name = "mp13-engine"
version = "0.9.0"
description = "HF-based Local LLM engine and tools"
authors = ["alexeiv-ai <88820640+alexeiv-ai@users.noreply.github.com>"]
readme = "README.md"
packages = [
    { include = "app", from = "src" },
    { include = "mp13_engine", from = "src" }
]

[tool.poetry.scripts]
mp13chat = "app.mp13chat:cli"
mp13config = "app.config:cli"

[tool.poetry.dependencies]
python = ">=3.12,<3.13"

python-dotenv = ">=0.19.0"
click = ">=8.0.0"
requests = ">=2.26.0"
pydantic = ">=2.4.2"

# Torch/CUDA anchor (Windows + Linux): keep pinned to avoid heavy dependency re-solving.
torch = { version = "==2.6.0+cu124", source = "pytorch-cuda" }
# HF stack anchor (transformers/peft/accelerate should be kept compatible)
transformers = "==4.53.1"
peft = ">=0.18.0,<0.19.0"
datasets = "^4.4.2"
accelerate = ">=1.12.0,<1.13.0"

# Optional perf/attention libs (torch/CUDA-locked)
xformers = { version = "*", optional = true }

# Torch-locked quantization utilities
torchao = "^0.13.0"
hqq = "^0.2.7"

# Loosely coupled utilities
numpy = ">=1.26.0"
tiktoken = "^0.9.0"

# Misc
matplotlib = "^3.10.1"
einops = "^0.8.1"
numexpr = "^2.11.0"
prompt-toolkit = "^3.0.51"

[tool.poetry.extras]
xformers = ["xformers"]

#
# Optional Triton support (single group, platform-selected via markers)
#
[tool.poetry.group.triton]
optional = true

[tool.poetry.group.triton.dependencies]
triton = { version = "==3.2.0", markers = "sys_platform != 'win32'" }
triton-windows = { version = "==3.2.0.post18", markers = "sys_platform == 'win32'" }


[tool.poetry.group.flashattn]
optional = true

# Includes the appropriate Triton package automatically (triton on Linux, triton-windows on Windows).

[tool.poetry.group.flashattn.dependencies]
# FlashAttention 2
# - Package URLs were found with https://flashattn.dev/versions
# - Community prebuilt wheels for PyTorch 2.6 (CUDA 12.4 / cu124)
flash-attn = [
  { url = "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.8.3+cu124torch2.6-cp312-cp312-win_amd64.whl", markers = "sys_platform == 'win32' and python_version == '3.12'" },
  { url = "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.16/flash_attn-2.8.3+cu124torch2.6-cp312-cp312-linux_x86_64.whl", markers = "sys_platform == 'linux' and platform_machine == 'x86_64' and python_version == '3.12'" },
  { url = "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.4/flash_attn-2.8.3+cu128torch2.6-cp312-cp312-linux_aarch64.whl", markers = "sys_platform == 'linux' and platform_machine == 'aarch64' and python_version == '3.12'" },
]

# Triton selection (FlashAttention backends depend on Triton)
triton = { version = "*", markers = "sys_platform != 'win32'" }
triton-windows = { version = "==3.2.0.post18", markers = "sys_platform == 'win32'" }

[[tool.poetry.source]]
name = "pytorch-cuda"
url = "https://download.pytorch.org/whl/cu124"
priority = "explicit"


[tool.poetry.group.dev.dependencies]
pytest = ">=6.2.5"
pytest-asyncio = ">=0.21.0"
pylint = "^3.3.4"

[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"

[tool.pytest]
testpaths = ["tests"]
python_files = ["test_*.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]
addopts = "-v --tb=short"
