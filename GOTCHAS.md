# Gotchas

Known caveats and behavior notes:

* **Multi-GPU behavior:** Multiple GPUs are supported, but this can negatively impact inference performance. Some models also do not behave well when sharded across GPUs. Unless you are constrained by VRAM, prefer device_map in config with a single explicit GPU like `cuda:0`.
* **Memory modes:** A workaround for `device_map='auto'` and  **meta/offload mismatch:** as a log line "Some parameters are on the meta device because they were offloaded to the cpu" followed by `NotImplementedError: Cannot copy out of meta tensor; no data!`. If the model fits, use `memory_mode=single_gpu` (fastest, simplest; warns on likely OOM). If it does **not** fit and **2+ GPUs** are installed, try explicit device map for those gpus, otheriwse  try `memory_mode=auto_cpu` which avoids meta tensors by loading fully into CPU first (requires **high CPU RAM** and slower load). **Default:** if you omit `memory_mode`, it uses `respect_device_map` (the engine won’t override your `device_map`; behavior depends on the model/Transformers).
* **Separate CUDA streams (use_separate_stream):** Leave this **enabled** for throughput unless you see degenerate outputs on a specific GPU/driver (often laptop hybrid/Optimus setups). If you observe repeating tokens like `0000…` or other nonsense only on CUDA (CPU OK), disable it in `engine_params` to force the default stream and retest.
* **Batch streaming:** HF does not support response streaming for batch requests. If the engine receives a streaming argument for a batch, it splits into the first prompt (streamed) and the rest of the batch (non-streamed). For higher TPS consider stream=False request arg when submitting a batch.
* **Static Cache limits:** As implemented, Static Cache does not yet deliver on full background warm-up and non blocking inference requests. Use it only for stable prompt shapes and after manual warm-up; speedups can reach about 50%. Concurrency has to be set to 1 explicitly.
* **Static Cache and FlashAttention:** `flash_attn_2` may not work with Static Cache depending on the model and other config options. Consider disabling static cache rather than going through troublshooting.
* **Quantization tradeoffs:** Quantizing base model on load (where supported) saves VRAM but usually hurts speed. Linux-only quantization schemes have not been tested yet but are planned before v1.0. The current plan is to support only Mixed PEFT-friendly schemes, which are limited today to HQQ.
* **Special tokens:** The engine trims only EOS and PAD tokens, leaving all other special tokens to the app's discretion.
* **PAD token behavior:** By default, the engine reserves a per-model stable PAD token if shipped model config sets PAD to EOS. This can be disabled, but there is evidence that removing PAD leads to LoRA adapter stability issues. I would rather not modify model PAD tokens without clear counter-arguments.
* **Builtin tools parsing:** This can be disabled in config or per request. The app layer can still implement the same with shipped parser outside of the engine logic.
* **Tools parsing on truncation:** When the engine detects a truncated response, the current app layer bails out so tool calls are not parsed/auto-executed even after recovereing full response with continuation (do_continue) engine feature. A manual app-layer call into the tools parser is required (not yet implemented). Note that response chunks always have parsed tool blocks cut off the chunks text while complete response text sent at the end does keep tool blocks as part of its content.
* **Adapter activation:** The engine supports global and per-request adapter activation. Both work, but concurrency may break semantics with global activation. Prefer per-request activation (no perf loss); use global adapters only if they will not change during a session or you know what you are doing.
* **Raw prompts:** The "raw prompt" engine feature has **not been tested** in a while hence could be broken for some time.
* **Generate args:** The engine does not expose non-standard args to HF `generate`, like `thinking=true`. Only what `GenerationConfig` supports is passed through.
* **WSL model paths:** When using WSL, keep models on the native Linux partition (not mounted) or model load times will suffer beyond reasonable.
* **Replay scope:** Only a subset of chat commands is replayed into the destination conversation. Most API-style commands and all log commands are ignored.
* **Replay permissions:** Global toolbox permission commands (for example, `/tools a`) are not replayed for safety when sharing sessions. In a source session, enable only needed tools in the global toolbox up front. In a destination session, allow only approved tools via config or manually before replaying a foreign session.
* **Replay flags:** Global flag commands are not replayed to allow source and destination configs to differ (for example, streaming or auto-truncation in the source can be suppressed in the destination).
* **Replay semantics:** Replay mainly supports commands that affect dynamic scoped resolution such as system messages, active adapters, and scoped tool permissions (subject to what the global toolbox exposes).
* **Replay comparisons:** A few API commands are supported to compare perf/memory metrics and raw prompts between source and destination. Metrics comparisons can be done with `/s hfs` run on the source and the replayed conversations.
